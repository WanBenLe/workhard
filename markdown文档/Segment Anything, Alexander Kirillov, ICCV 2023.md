## Segment Anything, Alexander Kirillov, ICCV 2023

图像编码器:生成C\*H\*W嵌入的编码器,使用ViT-H/16带14*14窗attention和4等距全局attention块.输出是1/16的嵌入计算是基于图像而不是每个提示的,所以只需要算一次.输入手心reshape成1024\*1024,嵌入是64\*64使用1\*1卷积得到256个通道后使用同样256通道的3\*3卷积+层归一化

提示编码器:嵌入到256dim,是位置编码和两个可学习嵌入(前景和背景)的和,Box是嵌入对,左上角额右上角嵌入,文本使用的是CLIP.Mask用1/4图像分辨率输入,接2*2的2步长卷积下采样.得到4和16通道的输出.最后1\*1卷积得到256dim结果.每层都有GELU和层归一化.如果没有mask就嵌入无mask的表征

轻量级mask解码器

学的输出标记嵌入插入到prompt emb,称为token.

1.token跑self-attention

2.查询token和图像emb的cross-attention

3.point-wise MLP更新token

4.查询的图像emb和token的cross-attention,使用prompt更新emb

cross-attention是64*64的256dim向量,attention和MLP用残差连接.层归一和0.1dropout

往下层解码层传递新token和图像emb,用了2层解码器.经过了attention就会有位置编码和原始提示token进入图像emb.使用2个转置卷积层变小了4倍.然后token再次参与图像emb并输出给3层MLP.放大图像emb和MLP输出的逐个元素相乘得到预测mask.Transformer是256dim,Transformer MLP是2048dim.

cross-attention有64*64的图像emb.attention是8头的.转置卷积是步长2的2\*2,输出通道是带GELU和层归一化的64和32.

模糊性问题:

单个输入prompt的模糊性,同时预测3个mask(整体.部分.子部分).只用最低损失进行更新.为了排名在附加输出token估计预测mask和覆盖对象的IoU.如果给出了多个提示只会预测一个mask(第四个预测的mask).

损失:

20*focal loss+1\*dice loss,IoU预测头使用预测和真实mask的IoU的MSE并使用1权重.

训练算法:

相同概率随机选择前景点/边界框,点从真实groud均匀采样,框是真实mask的框,并加入std=10%框边长并小于20像素的随机噪声.根据第一个prompt预测后从误差区域中选择后续点.假阴性:前景/假阴性:背景.使用上册mask预测作为额外prompt.掩码是未阈值mask的logits.多个编码是只传入预测IoU最高的mask.迭代共11次:初始输入,8个迭代和一次随机插入和一次在最后的迭代,后面两次不采样额外的点.

训练细节:

优化器使用AdamW($β_1$=0.9,$β_2$=0.999),线性学习率wram-up250iter和逐步的学习率衰减.初始学习率为$8*10^-4$.共SA-1B的2epoch即90k迭代,在60000和86000迭代时将lr减少10倍,batchsize=256,权重衰减=0.1,dp速率0.4.分层学习率衰减为0.8.没有数据增强.

ViT-H初始化SAM,图像编码器输入1024\*1024并使用256GPU训练.每个GPU最多64个随机采样的mask训练.丢弃了mask超过90%图像面积的图片.

# Segment Anything, Alexander Kirillov, ICCV 2023

三个互连组件来构建分割的基础模型：

1.可提示的分割任务

2.支持数据注释

3.支持零样本传输的分割模型 (SAM)

还有Segment Anything - 1 B,带10亿个mask的数据集



1.可提示的通用分割任务，可以提供强大的预训练目标并支持广泛的下游应用程序。

2.支持灵活提示的模型，并能在提示时实时输出分段掩码以允许交互使用。 

3.多样化的大规模数据源, 构建了一个“数据引擎”，即我们在使用高效模型协助数据收集和使用新收集的数据改进模型之间进行迭代



任务:提示只是指定在图像中分割什么.有效输出掩码的要求意味着，即使提示不明确并且可能引用多个对象,输出也应该是合理的掩码 至少其中一个对象。 我们使用提示分割任务作为预训练目标，并通过提示工程解决一般下游分割任务.

模型:模型必须支持灵活的提示，需要实时摊销计算掩码以允许交互使用，并且必须具有模糊性意识。SAM:图像编码器计算图像嵌入，提示编码器嵌入提示，然后将两个信息源组合在预测分割掩模的轻量级掩模解码器中。 SAM可以通过不同的提示重复使用相同的图像嵌入（及其成本摊销 cost amortized）. 将SAM设计为预测单个提示的多个掩码，从而使 SAM 能够自然地处理歧义

数据引擎:数据引擎分为三个阶段：辅助手动、半自动和全自动。 在第一阶段，SAM 协助注释者注释掩模，类似于经典的交互式分割设置。 在第二阶段，SAM 可以通过提示可能的对象位置来自动为对象子集生成掩码，而注释器则专注于注释其余对象，从而帮助增加掩码多样性。 在最后阶段，我们使用前景点的规则网格提示 SAM，平均为每个图像生成 100 个高质量掩模

数据集:  SA-1B 包含来自 1100 万张许可和隐私保护图像的超过 1B 个掩模. SA-1B 使用我们的数据引擎的最后阶段完全自动收集，比任何现有的分割数据集多了 400 个掩模，这些掩模具有高质量和多样性 。 

负责任的人工智能:SA-1B 中的图像跨越了地理和经济上不同的国家，我们发现 SAM 在不同人群中的表现相似.

## SAM模型



图像编码器

MAE预训练的ViT-H在提示编码器前运行.

提示编码器

考虑两组提示:稀疏(点.框.文本)和密集的mask. 我们通过位置编码来表示点和框,并使用 CLIP 中现成的文本编码器对每种提示类型和自由格式文本进行学习emb. 使用卷积嵌入mask,并与图像嵌入按元素求和.

mask解码器

mask解码器有效地将图像嵌入.提示嵌入和输出标记映射到掩码.该设计采用了 Transformer 解码器块的修改,后跟动态掩模预测头. 修改后的解码器块使用两个方向的即时自注意力和交叉注意力来更新所有嵌入.运行后我们对图像嵌入进行上采样,并且 MLP 将输出标记映射到动态线性分类器,然后计算每个图像位置的掩模前景概率。 

解决歧义

对于一个输出,如果给出不明确的提示,模型将对多个有效掩码进行平均.为了解决这个问题，我们修改模型以预测单个提示的多个输出掩码. 我们发现 3 个掩码输出足以解决最常见的情况（嵌套掩码通常最多三层深度：整体、部分和子部分）. 在训练期间，我们仅反向传播掩模上的最小损失。 为了对掩模进行排名，模型预测每个掩模的置信度得分（即估计的 IoU）。 

效率

在 Web 浏览器中的 CPU 上运行，耗时 50 毫秒。 这种运行时性能可以为我们的模型提供无缝、实时的交互式提示。

损失和培训

我们使用中使用的Focal损失和Dice损失的线性组合来监督掩模预测。 我们使用几何提示的混合来训练可提示的分割任务。 我们通过在每个掩码 11 轮中随机采样提示来模拟交互式设置，从而使 SAM 能够无缝集成到我们的数据引擎中。



SAM数据引擎

辅助手动阶段

专业注释者团队使用由 SAM 提供支持的基于浏览器的交互式分割工具，通过单击前景/背景对象点来标记蒙版. 没有对标记对象施加语义约束，注释者可以自由地标记“东西”和“事物”. 我们建议注释者标记他们可以命名或描述的对象，但不收集这些名称或描述。 注释者被要求按照突出的顺序标记对象，并被鼓励在蒙版注释时间超过 30 秒后继续处理下一张图像。开始时，使用常见的公共分割数据集对 SAM 进行了训练。 经过足够的数据注释后，仅使用新注释的掩模重新训练 SAM。 随着收集到的掩模越来越多，图像编码器从 ViT-B 扩展到 ViT-H，并且其他架构细节也不断发展； 我们总共重新训练了模型 6 次。 在此阶段从 12 万张图像中收集了 430 万个掩模。 

半自动阶段

 为了将注释者集中在不太突出的对象上，我们首先自动检测置信蒙版。 然后，我们向注释者提供了预先填充了这些蒙版的图像，并要求他们注释任何其他未注释的对象。 为了检测置信的掩模，我们使用通用的“对象”类别在所有第一阶段掩模上训练了边界框检测器. 额外收集了 590 万个掩模（总共 1020 万个掩模）。 我们定期根据新收集的数据重新训练我们的模型（5 次）。

全自动阶段

收集了足够的掩模来极大地改进模型，包括上一阶段的各种掩模。开发了歧义感知模型，即使在歧义情况下，它也使我们能够预测有效的掩码。使用 32*32 个规则点网格来提示模型，并为每个点预测一组可能对应于有效对象的掩模。 使用歧义感知模型，如果一个点位于部分或子部分上，我们的模型将返回子部分、部分和整个对象。 我们模型的 IoU 预测模块用于选择置信掩码； 此外，我们仅识别和选择稳定的掩模（如果将概率图阈值设置为 0.5+-sigma 会产生类似的掩模，则我们认为掩模是稳定的）。 在选择置信且稳定的掩模后，我们应用非极大值抑制（NMS）来过滤重复项。 为了进一步提高较小蒙版的质量，还处理了多个重叠的放大图像裁剪.



数据集SA-1B

11M高分辨率图像个1.1B个自动生成的mask1100w张高分辨率图像,进行下采样使得最短边变为1500像素.图片高质量但存在对性别.年轻人.深色皮肤的偏见.



zero-shot

让SAM:a.边缘检测b.分割所有内容c.分割检测的对象d.分割自由格式文本对象

SAM使用MAE预训练的ViT-H,在SA-1B上训练

1.从单个前景点分割对象,使用mIoU指标(预测和真实mask的IoU均值),SAM使用最置信的那个mask预测

2.边缘检测,16*16前景点规则网格提示SAM产生768预测mask,冗余的NMS删除.使用Sobel滤波和标准轻量级后处理计算边缘图.提高了recall降低了精确度(劣于HED边缘抑制模型)

3.对象提议,用自动mask生成的微调版本将mask作为建议输出,在中大型物体上表现更好.(劣于ViT-Det-H)

4.实例分割,使用对象检测器ViTDet提示SAM.Mask质量更优更清晰但不如ViTDet,但在人类评分上表现更好

5.自由格式文本分割对象

手动搜集面积大于1002的mask使用CILP(处理了图像-文本对齐)提取图像嵌入,然后传入SAM进行第一次交互.SAM的prompt使用CILP的文本嵌入生成得到



