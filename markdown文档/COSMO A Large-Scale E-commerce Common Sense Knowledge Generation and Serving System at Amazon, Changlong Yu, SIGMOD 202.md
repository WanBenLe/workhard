#### COSMO: A Large-Scale E-commerce Common Sense Knowledge Generation and Serving System at Amazon, Changlong Yu, SIGMOD 2024

电商平台中的知识图谱KG虽然集成了大量的概念或产品属性,但无法发现用户的意图,因此提出了从海量行为中挖掘以用户为中心的常识知识,并构建行业规模的知识图谱来赋能多样化的可拓展在线服务COSMO:从LLM提取种子知识断言并使用人机循环注释数据训练的批评分类器细化的pipeline, 并描述了如何prompt微调COSMO-LM以大规模生成忠实的电子商务常识知识.



##### 用户行为

搜索-购买:查询-产品对$(𝑞,𝑝)$即客户点击查询𝑞并最终在短时间内购买产品𝑝

共同购买:共同购买产品对$(𝑝_1,𝑝_2)$来表示共同购买行为。 

每个产品𝑝都可以分为一个主要领域$𝑑 \in D$

常识性知识:利用LLM的关系感知提示将用户行为ℎ解释为知识候选,将知识表示为三元组$(ℎ,𝑟,𝑡)$，其中𝑟和𝑡分别表示关系和尾部

使用4个种子关系used for/capable of/is a/cause开始,并挖掘频繁谓词模式来手动总结关系,下表是一些示例

![image-20240401152503385](C:\Users\SFC\AppData\Roaming\Typora\typora-user-images\image-20240401152503385.png)

prompt:一组指令$\{I_t\}$用NLP定义任务$t$,每个任务包含$l_t$对输入-输出实例

输入是用户行为对$(𝑞,𝑝)$或$(𝑝_1,𝑝_2)$,输出是知识尾t,(h,r,t)通过人类打标合理性和典型性分数判断,并使用高典型性分数知识作为模型输出,添加了一些辅助任务并训练了用于知识泛化和在线服务的语言模型.



##### 知识生成

###### 用户行为采样

![image-20240402145907109](C:\Users\SFC\AppData\Roaming\Typora\typora-user-images\image-20240402145907109.png)

1.产品抽样:热门品类+高交互产品

2.品类+类型标签(有点像抽产品名之后的玩意)

3.共同购买每个边缘至少选一个,消除随机共同购买避免重复抽样

4.启发式规则:产品共同购买要求同品类 

5.搜索-购买对抽样基于购买率和点击率的阈值过滤,并要求查询与购买产品相关

6.抽样参与度较低和购买率较低的查询



###### QA提示生成

提供任务描述并遵循具体查询和产品信息,可以附加一个问题和部分答案,使用OPT 175b和OPT 30b生成

![image-20240403092326791](C:\Users\SFC\AppData\Roaming\Typora\typora-user-images\image-20240403092326791.png)

##### 知识细化

###### 粗粒度过滤

nltk分句得到第一句并使用GPT2得到困惑度分数,进而根据阈值删除不完整的句子.

过滤查询/产品类型/产品标题满足完全相同/编辑距离小于阈值的item.

一般性(非特定查询)知识基于频率和熵识别.

###### 相似性过滤

内部语言模型,该模型在电子商务语料库(包括查询.产品信息等)上进行预训练,以获得用于生成知识的嵌入.

查询和产品的emb,相似度使用cosSim计算$d(k,c)=cos(E(k),E(c))$

过滤后LLM本质是句法转换后的原始用户行为上下文释义.

###### 人机交互注释

由于均匀采样会损害长尾知识的预测性能,使用知识频率日志和产品/查询流行度重新加权

$w_{(q,p),t}=\frac{log(f(t))}{pop(q)pop(p)}$,$f(t)$​是生成知识的频率,越受欢迎的知识可能是越通用的.

2个人label回答五个问题(是/否/不确定)并第三人确定是否分歧.

1.解释是完整句子吗?

2.解释相关吗?

3.解释内容丰富吗?

4.解释可信吗?

5.解释典型吗?

用标注后结果建立内部语言模型和DeBERTa-large分类模型,并将合理性得分大于0.5的保留



##### prompt微调的COSMO-LM

LLM的任务

1. LLM能发现共同购买的的共同原因

2. LLM具有合理性和典型性预测的能力,微调数据为:所有的人工注释数据
3. LLM发现用户行为数据的不可忽略的噪声,微调数据为:细粒度注释已经识别出不相关的查询产品对或随机购买对
4. 共同购买预测
5. 搜索相关性预测

最后有18产品域.15关系类型.5种不同任务的prompt数据,在不同任务前给不同指令(不同的prompt模版),使用LLaMA 7b和LLaMA 13b微调

![image-20240403095224412](C:\Users\SFC\AppData\Roaming\Typora\typora-user-images\image-20240403095224412.png)

##### 在线部署

Sage-Maker部署:处理用户行为会话日志和动态模型更新

请求处理:针对异步缓存存储进行初始查询检查,快速检索频繁查询的响应或转发其他查询以进行批处理

批处理和缓存更新:功能存储将语言模型响应格式化为结构化见解,更新缓存以供将来查询

与下游应用程序的通信:来自缓存的结构化数据增强了各种下游应用程序,提供了丰富的功能以改进用户交互

反馈循环:通过将用户交互反馈到COSMO-LM 中来实现持续的模型细化,确保对不断变化的用户行为做出最新的响应

天级更新的话对限时活动之类的不够及时



##### COSMO-LM应用

###### 搜索相关性评价

KDD Cup 2022 公开发布的亚马逊购物查询数据集

将给定产品和匹配,精确/替代/补充/不相关的四分类

预测使用BI-encoder和cross-encoder,模型使用deberta-v3-large,输入为[Q,P,G]

1.有限注释 COSMO-LM 也能增强cross-encoder性能

2.意图增强的交叉编码器模型可以显着优于所有语言环境的基线方法

在线部署环境中,生成的知识以及存储在特征存储中的其他特征被集成以做出最终预测,并为频繁的搜索查询预先缓存特征



###### 会话推荐改进

GCE-GNN聚合了来自会话图和具有soft-attn的全局图的两个级别的项目嵌入

对于会话$𝑆$中的时间$𝑡$,用户搜索查询$𝑘^{𝑠}_𝑡$并与项目 $𝑣^𝑠_𝑡$ 进行交互,并从 GCE-GNN 获得的项嵌入表示为$h^𝑠_𝑡 $.然后使用 COSMO-LM 生成意图知识并获得嵌入的会话知识$g^𝑠_𝑡$ ,再使用两层感知机进行特征空间对齐将知识表示$g^𝑠_𝑡$转换为 $\hat{g}^𝑠_𝑡$ ,最后concat得到结果.



###### 搜索导航

基于粗粒度知识扩展(露营-充气床垫-露营充气床垫-湖边露营充气床垫/山地露营充气床垫)

1.多转弯导航

2.产品类型/子类型发现

3.个性化过滤