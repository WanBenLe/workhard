##### Transformers

##### self-attn

$softmax(mask((QK^\top)/\sqrt{d_k}))V$

除以~是为了防止方差过大(标准化)和梯度过大

##### MHA

仅用于当前位置编码,会focus在学习自身位置,因此通过MHA进行多次self-attn后linear的结果concat

$MultiHead(Q,K,V)=Concat(head_1,...,head_n)$

Encoder在MHA后接两层MLP:Dropout(LayerNorm(x+Sublayer(x))),第一层接ReLu激活

##### position emb

引入用于获得时序信息

$PE_{pos,2i}=sin(pos/10000^{2i/d_{model}})$

$PE_{pos,2i+1}=cos(pos/10000^{2i/d_{model}})$

##### Decoder

会用到encoder linear后的K,V和自己的Q算MHA

##### Mask

使用右上三角的attn-mask防止注意到未来的信息.

Padding Mask防止padding长度处理后关注到了padding的信息



##### BERT

Encoder Only架构,跟ChatGLM一样

输入是两段([CLS],...,[SEP],...[EOS])三部分,token,sents,绝对位置编码,最多512token

emb靠学习,相加后标准化就是emb.

MLM:选15% token,80%[mask],10%随机换,10%不变并预测

NSP:预测下一句是否为真实的下一句.

Adam的$\beta_1=0.9,\beta_2=0.999,\epsilon=1e-6$学习率衰减$L_2=0.01$

预热1e-4学习率线性衰减10000steps,给dropout=0.1,GELU激活,更新100w次,bs=256,16g文本



##### RoBERTa

更大的batchsize(8 K),更多的数据160 G文本,设置$\beta_2=0.98$

删除下一个句子是否为下一句的预测目标,使用Full-Sentence:每个输入包含从一个或多个文档中连续采样不多于512 token的完整句子,跨越文档进行采样时添加额外分隔符.

50 k 字节对编码BPE词表

动态修改mask模式:训练数据复制10次,每次的mask是不同的(共40次里会有4次重复)



##### DeBERTa

encoder-decoder架构

Electra-Style的DeBERTa-V3

生成器(MLM,参数G)生成模糊token并替换原序列得到新序列,并训练二分类鉴别器(RTD,参数D)预测token是被替换的标记还是原始输入.

$L_{MLM}=E(-\sum_{i\in C}log \space p_{\theta_G}(\tilde{x}_{i,G}=x_i|\tilde{X}_G))$

$\tilde{X}_G$是生成器的输入,被随机mask的15% token,鉴别器的输入通过生成器输出概率采样的新token替换mask后token来构建.

$\tilde{x}_{i,D}= \tilde{x}_{i} \sim p_{\theta_G}(\tilde{x}_{i,G}=x_i|\tilde{X}_G)), \space i \in C$

$\tilde{x}_{i,D}= {x}_{i} , \space i \notin C$​

$L_{RTD}=E(-\sum_{i}log \space p_{\theta_D}(1(\tilde{x}_{i,D}=x_i)|\tilde{X}_D,i))$​

$L=L_{MLM}+L_{RTD}$​

2.鉴别器和生成器共享token emb(不共享会导致性能下降),引入梯度解缠嵌入共享GDES处理因为鉴别器和生成器的训练损失将标记嵌入拉向不同的方向的冲突优化问题,sg为停止梯度传播.

$E_D=sg(E_G)+E_{\Delta}$​



##### AWP

保存梯度后做随机微小扰动,step后恢复梯度,可以让NLP模型更robust



##### Multi-Sample Dropout

在输出结果前接0.1-0.5的5个不同的dropout得到不同的结果,然后mean求均值

类似于SimCSE的同样本dropout低成本加快收敛速度.



##### Asymmetric Loss

Focal Loss:$L_{+}=(1-p)^{\gamma}log(p)$

基于Focal Loss的改进,为了避免$gamma$过大使得稀缺正样本梯度过低,将正负样本$gamma$分开,并且将prob低于硬阈值的压缩,可以视为处理错误标注样本

$L_{+}=(1-p)^{\gamma_+}log(p)$​

$L_{-}=p_m^{\gamma_-}log(1-p)$​

自适应超参$\gamma$:定义概率差距$\Delta_p=p^+_t-p^-_t$ ,后者为正负样本的平均概率.

$\gamma_-=\gamma_-+\lambda(\Delta_p-\Delta_{p_{target}})$

其中$\lambda$是步长,$\Delta_{p_{target}}$​设为0.1



##### 冷启动/跨域

1.使用同品类数据均值替换

2.基于相似性mapping已有数据

3.基于LLM生成相关数据



##### InfoNCE

温度系数越大越容易收敛,效果越差

温度系数越小越容易尖锐,注重难样本,但是难收敛,如果分布偏移了泛化效果差



##### ESimCSE

SimCSE会认为样本长度类似的有高相似性,随机插入和删除可能会引入噪音,使用随机词重复增加长度

SimSCE batchsize虽然越高越好但是太大影响性能,ESimCSE引入了动量对比:维护对比学习的emb队列,并删除最旧的emb,使得多了一个平滑的encoder有参数$\theta_m$反向传播只更新$\theta_e$

$\theta_m \leftarrow\lambda\theta_m+(1-\lambda)\theta_e,\lambda\in [0,1)$

loss从原本的InfoNCE变为

$L_i=-log\frac{e^{sim(h_i,h_i^{+})}/\gamma}{\sum_{j=1}^{N}e^{sim(h_i,h_j^{+})}/\gamma+\sum_{m=1}^{N}e^{sim(h_i,h_m^{+})}/\gamma}$



##### AngIE

监督的对比学习标签一般都是0或者1,cos(1)和cos(-1)容易导致梯度消失,引入复空间的角度优化loss.

$L_{cos}=log[1+\sum_{s(X_i,X_j)>s(X_m,X_n)}e^{\frac{cos(X_m,X_n)-cos(X_i,X_j)}{\gamma}}]$

Cosine目标要求高相似对的cos大于低相似的cos

$L_{ibn}=-\sum_{b}\sum_{i}log[\frac{e^{cos(X_{b_i},x^+_{b_i})/\gamma}}{\sum_j^N{e^{cos(X_{b_i},x^+_{b_j})/\gamma}}}]$

批内负目标的正样本是数据增强的结果,负样本是prompt LLM生成的(如果用LLM做为基座)

实际处理是分块,先分成两部分,然后再分成两部分:前半部分表征实数,后半部分表征复数

$z=a+bi \in C, w=c+di \in C$, a. b. c. d都是实数,进而有:

$\Delta \theta_{zw}=(\theta_z-\theta_w)=\frac{z}{w}/\gamma=abs[\frac{(ac+bd)+(bc-ad)i}{\sqrt{(c^2+d^2)(a^2+b^2)}}]$

$L_{angle}=log[1+\sum_{s(X_i,X_j)>s(X_m,X_n)}e^{\frac{\Delta \theta_{ij}-\Delta \theta_{mn}}{\gamma}}]$

$L=\omega_1\cdot L_{cos}+\omega_2\cdot L_{ibn}+\omega_3\cdot L_{angle}$

