# Counterfactual harm

1.使用因果模型提出了第一个正式的危害和收益定义

2.任何关于伤害的事实定义都无法识别某些情况下的有害行为,无法执行反事实推理的标准机器学习算法一定会在分配变化后采取有害政策

3.设计了一个使用反事实目标函数的避免伤害决策的框架并使用从RCT数据中学习到的处理反应模型来证明.

4.提出的反事实方法则确定了在不牺牲处理效应的情况下选择了危害明显较小的处理



如果一个处理存在收益和危害,在同样的损益期望下策略无法区分,**但基于伦理学,对于治疗而言会优先选择危害更小的.**



结构因果模型SCM

$M=<\mathbb{g},E,X,F,P>$

1.内生变量$X=\{X^1,..,X^N\}$

2.服从联合分布$P(E=e)=\prod_{i=1}^{N}P(E^i=e^i)$的.相互独立外生变量$E=\{E^1,..,E^N\}$

3.有向无环图$\mathbb{g}$,懒得写了,就是因果图

4.将E mapping到X的机制F,$x^i=f_i(pa_i,e^i)$

于是有$P(X=x)=\int_{e:X(e)=x}P(E=e)$

SCM给定了联合分布的同时,也给定了只需要更新的do的干预分布,于是

条件平均处理效应$CATE(x)=E[Y_{T=1}|x]-E[Y_{T=0}|x]$

反事实概率$PN=P(Y_{X=F}=F|X=T,Y=T)$虽然无法单独得出,但是可以在SCM框架下计算

$\begin{flalign}&P(Y_{X=F}=F|X=T,Y=T)=\\&\int_{Y_{X=F(e)}=F,Y(e)=T,X(e)=T}P(E=e)\end{flalign}$

反事实比较解释CCA:事件 e 或行动 a 总体上对一个人造成伤害/好处,当且仅当如果 e 没有发生/a没有执行,ta的平衡状况会更好/更差

医学中可能会用安慰剂去替代CCA中的没有发生/没有执行

![image-20230706171446072](C:\Users\SFC\AppData\Roaming\Typora\typora-user-images\image-20230706171446072.png)

一个基于行动a的期望函数如下所示,U是用户的效用函数

$E[U|a,x]=\int_{y}P(y|a,x)U(a,x,y)$

最优行动为$a_{max}=\arg \max_{a}E[U|a,x]$

假设一个二元的行动,有$a_{max}=\arg \max_{a}\{E[Y_a|x]-E[Y_0|x]\}$,即最大化CATE

与$A=\bar{a}$相比$A=a$的反事实伤害为

$h(a,x,y;M)=\int_{y^*}P(Y_{\bar{a}}=y^*|a,x,y;M)\max\{0,U(\bar{a},x,y^*-U(a,x,y))\}$

预期收益为

$b(a,x,y;M)=\int_{y^*}P(Y_{\bar{a}}=y^*|a,x,y;M)\max\{0,U(a,x,y)-U(\bar{a},x,y^*)\}$

损益权衡:

$E[U|a,x]-E[U|\bar{a},x]=E[b|a,x;M]-E[h|a,x;M]$

对于上面的二元处理,有反事实伤害$E[h|a;M]=P(Y_0=1,Y_a=0;M)$,结果证明依赖于一些因果推断的假设,从略

如果存在反事实伤害,需要更高的期望收益才愿意做出决策的成为伤害厌恶,据此有反事实目标函数-伤害厌恶效用HPU

$V(a,x,y;M)=U(a,x,y)-\lambda h(a,x,y;M) \lambda$是伤害厌恶的系数

相关的定义和分析可以意会金融的损失厌恶,从略

例子3以代理人悖论陈述了基于投资者已知信息下代理人的3个行为

1.最大化期望收益,但产生了风险与伤害

2.风险厌恶,最小化了风险(事实目标),但是产生了伤害

3.伤害厌恶

说明了基于事实目标的决策行为没法很好的捕捉到用户本身的偏好

对于一个给定的事实目标函数(效用函数.成本函数等)$J(a,x,y)$,有可以仅使用数据估计的最佳动作

$a_{max}=\arg \max_{a}E[J|a,x]=\arg \max_{a}\int_{y}P(y|a,x;M)J(a,x,y)$

有害行动及策略:若行动a效用不大于行动a',且反事实伤害大于a',则称为行动a严格有害,若策略对于严格有害的行动分配了大于0的概率,则该策略是严格有害策略

有害目标:如果最大化目标的期望值会产生严格有害策略,则称该目标为严格有害目标

对于任何效用函数U.环境M和默认操作A =a预期HPU 对于任何 > 0 都不是严格有害的目标

结果分布偏移:SCM下外生噪声分布发生变化和/或因果机制发生变化

$P(e^Y;M')\neq P(e^Y;M)$ and/or$ f'_Y(a,x,e^Y)\neq f_Y(a,x,e^Y)$

结果依赖:$\forall a_i,a_j \in C,i\ne j, max_yU(a_i,x,y)>min_yU(a_j,x,y)$

结果独立代表着不需要学习且效用独立于结果分布

如果存在结果分布偏移,对于结果依赖的U,U在结果分布偏移后的环境中是严格有害的

对于环境X=x,如果U结果依赖于操作a和另外2个操作$a_1,a_2$,那么事实目标函数J在结果分布偏移的环境是严格有害的

在实际策略中,加入对分布偏移的罚项会有利于伤害控制